{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = spacy.load('en_core_web_trf')\n",
    "\n",
    "\n",
    "def parse_data(data: dict) -> list:\n",
    "    words = data['tokens']\n",
    "    doc = Doc(parser.vocab, words=words)\n",
    "    tokens = parser(doc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def clean_data(dataset: list) -> list:\n",
    "    # remove mis-annotated entity with PUNCT as ending word\n",
    "    # such as [Dec, 12, .] should be [Dec, 12] with . removed\n",
    "    for idx, cur_d in enumerate(tqdm(dataset)):\n",
    "        words = cur_d['tokens']\n",
    "        doc = Doc(parser.vocab, words=words)\n",
    "        tokens = parser(doc)\n",
    "        for ent in cur_d['entities']:\n",
    "            ent_words = cur_d['tokens'][ent['start']: ent['end']]\n",
    "            if len(ent_words[-1]) == 1 and tokens[ent['end'] - 1].pos_ == 'PUNCT':\n",
    "                ent['end'] -= 1\n",
    "                print('fixed ----post---->', idx, ent_words)\n",
    "            if len(ent_words[0]) == 1 and tokens[ent['start']].pos_ == 'PUNCT':\n",
    "                ent['start'] += 1\n",
    "                print('fixed ----pre----->', idx, ent_words)\n",
    "\n",
    "        new_words = []\n",
    "        for idx, word in enumerate(words):\n",
    "            if len(word) > 1 and word[0] == ',':\n",
    "                new_words.append(',')\n",
    "                new_words.append(word[1:].strip())\n",
    "                print('fixed -----starting comma------->', idx, word)\n",
    "                for ent in cur_d['entities']:\n",
    "                    if ent['start'] >= idx:\n",
    "                        print('move ent start from ', ent['start'], ' to ', ent['start'] + 1)\n",
    "                        ent['start'] += 1\n",
    "                    if ent['end'] >= idx:\n",
    "                        print('move ent end from ', ent['end'], ' to ', ent['end'] + 1)\n",
    "                        ent['end'] += 1\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        cur_d['tokens'] = new_words\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_reachable_tokens(tokens: list):\n",
    "    token_dic = {}\n",
    "    post_token_dic = {}\n",
    "    token_idx_dic = {}\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token_idx_dic[token] = idx\n",
    "        token_dic[token] = [token]\n",
    "        post_token_dic[token] = [token]\n",
    "    for token in tokens:\n",
    "        cur_token = token\n",
    "        while cur_token.head != cur_token:\n",
    "            if token_idx_dic[token] < token_idx_dic[cur_token.head]:\n",
    "                # token can be reached by heads\n",
    "                # this if ensure only previous words are reached\n",
    "                token_dic[cur_token.head].append(token)\n",
    "            else:\n",
    "                post_token_dic[cur_token.head].append(token)\n",
    "            cur_token = cur_token.head\n",
    "\n",
    "    for key, tokens in token_dic.items():\n",
    "        token_dic[key] = sorted(set(tokens), key=lambda token: token_idx_dic[token], reverse=True)\n",
    "    for key, tokens in post_token_dic.items():\n",
    "        post_token_dic[key] = sorted(set(tokens), key=lambda token: token_idx_dic[token])\n",
    "    return token_dic, post_token_dic, token_idx_dic\n",
    "\n",
    "\n",
    "def is_valid_ent_end_token(token):\n",
    "    # return is_valid, is_expandable (if it could be included in spans with multiple words)\n",
    "    if token.pos_ in ['PROPN', 'NUM', 'NOUN']:\n",
    "        return True, True\n",
    "    if token.pos_ == 'ADJ' and token.text[0].isupper():\n",
    "        # ADJ entities are alway single word\n",
    "        return True, False\n",
    "    if token.pos_ == 'PRON':\n",
    "        return True, False\n",
    "    return False, False\n",
    "\n",
    "\n",
    "def is_valid_ent_start_token(token):\n",
    "    # return is_valid\n",
    "    if token.pos_ in ['PROPN', 'NUM', 'SYM']:\n",
    "        return True\n",
    "    if token.pos_ in ['NOUN', 'PRON']:\n",
    "        return True\n",
    "    if token.pos_ == 'ADJ':\n",
    "        return True\n",
    "    if token.pos_ == 'DET' and token.text[0].isupper() and token.text.lower() == 'the':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_valid_ent_start_token_v1(token):\n",
    "    # works well in general domaind datasets such as CONLL\n",
    "    # return is_valid\n",
    "    if token.pos_ in ['PROPN', 'NUM', 'SYM']:\n",
    "        return True\n",
    "    if token.pos_ == 'NOUN' and token.text[0].isupper():\n",
    "        return True\n",
    "    # if token.pos_ == 'VERB' and token.text[0].endswith('ed'):\n",
    "    #     return True\n",
    "    if token.pos_ == 'ADJ' and token.text[0].isupper():\n",
    "        return True\n",
    "    if token.pos_ == 'DET' and token.text[0].isupper() and token.text.lower() == 'the':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_valid_punct_span(span):\n",
    "    VALID_PUNCTS = [',', '(', ')']\n",
    "    left_par_cnt = 0\n",
    "    right_par_cnt = 0\n",
    "    for token in span:\n",
    "        if token.pos_ == 'PUNCT' and token.text not in VALID_PUNCTS:\n",
    "            return False\n",
    "        if token.text == '(':\n",
    "            left_par_cnt += 1\n",
    "        if token.text == ')':\n",
    "            right_par_cnt += 1\n",
    "    if left_par_cnt != right_par_cnt:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_valid_span(span):\n",
    "    MAX_SPAN_LEN = 10\n",
    "    if not is_valid_ent_start_token(span[0]):\n",
    "        return False\n",
    "    \n",
    "    if len(span) > MAX_SPAN_LEN:\n",
    "        return False\n",
    "    \n",
    "    if not is_valid_punct_span(span):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_ent_spans(tokens: list) -> set:\n",
    "    candidates = []\n",
    "    token_dic, post_token_dic, token_idx_dic = get_reachable_tokens(tokens)\n",
    "    for token in tokens:\n",
    "        # generate spans that only include words before token\n",
    "        candidates.extend(get_token_spans(token, token_dic, token_idx_dic))\n",
    "        # generate spans that contains words after token\n",
    "        candidates.extend(get_post_token_spans(token, post_token_dic[token], token_dic[token], token_idx_dic))\n",
    "\n",
    "    candidates.extend(get_date_spans(tokens))\n",
    "    candidates.extend(get_loc_spans(tokens))\n",
    "    # convert to (span_start, span_end) format\n",
    "\n",
    "    candidates = [can for can in candidates if is_valid_span(can)]\n",
    "\n",
    "    candidates = [(token_idx_dic[can[0]], token_idx_dic[can[-1]]+1) for can in candidates]\n",
    "    return set(candidates)\n",
    "\n",
    "\n",
    "def get_post_token_spans(token, post_tokens, pre_tokens, token_idx_dic):\n",
    "    # try to make it work on sci domain datasets such as scierc\n",
    "    # if token.pos_ != 'PROPN':\n",
    "        # only PROPN followed by ADP can generate valid spans\n",
    "        # return []\n",
    "    \n",
    "    filtered_post_tokens = []\n",
    "    \n",
    "    for idx, p_token in enumerate(post_tokens):\n",
    "        if token_idx_dic[p_token] - token_idx_dic[token] != idx:\n",
    "            break\n",
    "        filtered_post_tokens.append(p_token)\n",
    "    \n",
    "    # if len(filtered_post_tokens) < 3 or filtered_post_tokens[1].pos_ != 'ADP':\n",
    "        # only PROPN followed by ADP can generate valid spans\n",
    "        # should be at least \"A of B\", where of can be other ADP words\n",
    "        # return []\n",
    "    \n",
    "    candidates = [filtered_post_tokens]\n",
    "    # cur_ent = post_tokens[:2]\n",
    "    # could be multiple ADP, segment each of them\n",
    "    for idx in range(2, len(filtered_post_tokens)):\n",
    "        if filtered_post_tokens[idx].pos_ != 'PROPN':\n",
    "            candidates.append(filtered_post_tokens[:idx])\n",
    "    \n",
    "    filter_pre_tokens = []\n",
    "    for idx in range(1, len(pre_tokens)):\n",
    "        if pre_tokens[idx].pos_ != 'PROPN':\n",
    "            break\n",
    "        filter_pre_tokens = [pre_tokens[idx]] + filter_pre_tokens\n",
    "    \n",
    "    candidates = [filter_pre_tokens + can for can in candidates if can[-1].pos_ in ['PROPN', 'NOUN']]\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_post_token_spans_v1(token, post_tokens, pre_tokens, token_idx_dic):\n",
    "    # works well for general domain datasets such as CONLL\n",
    "    if token.pos_ != 'PROPN':\n",
    "        # only PROPN followed by ADP can generate valid spans\n",
    "        return []\n",
    "    \n",
    "    filtered_post_tokens = []\n",
    "    \n",
    "    for idx, p_token in enumerate(post_tokens):\n",
    "        if token_idx_dic[p_token] -  token_idx_dic[token] != idx:\n",
    "            break\n",
    "        filtered_post_tokens.append(p_token)\n",
    "    \n",
    "    if len(filtered_post_tokens) < 3 or filtered_post_tokens[1].pos_ != 'ADP':\n",
    "        # only PROPN followed by ADP can generate valid spans\n",
    "        # should be at least \"A of B\", where of can be other ADP words\n",
    "        return []\n",
    "    \n",
    "    candidates = [filtered_post_tokens]\n",
    "    # cur_ent = post_tokens[:2]\n",
    "    # could be multiple ADP, segment each of them\n",
    "    for idx in range(2, len(filtered_post_tokens)):\n",
    "        if filtered_post_tokens[idx].pos_ != 'PROPN':\n",
    "            candidates.append(filtered_post_tokens[:idx])\n",
    "    \n",
    "    filter_pre_tokens = []\n",
    "    for idx in range(1, len(pre_tokens)):\n",
    "        if pre_tokens[idx].pos_ != 'PROPN':\n",
    "            break\n",
    "        filter_pre_tokens = [pre_tokens[idx]] + filter_pre_tokens\n",
    "    \n",
    "    candidates = [filter_pre_tokens + can for can in candidates]\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_token_spans(token, token_dic, token_idx_dic):\n",
    "    candidates = []\n",
    "    is_ent, expandable = is_valid_ent_end_token(token)\n",
    "    if is_ent:\n",
    "        candidates.append([token])\n",
    "    if expandable:\n",
    "        cur_ent = [token]\n",
    "        cur_idx = 1\n",
    "        while cur_idx < len(token_dic[token]):\n",
    "            if cur_idx < len(token_dic[token]) and token_idx_dic[token_dic[token][cur_idx]] + 1 == token_idx_dic[cur_ent[0]]:\n",
    "                candidates.append([token_dic[token][cur_idx]] + cur_ent)\n",
    "                cur_ent = [token_dic[token][cur_idx]] + cur_ent\n",
    "                cur_idx += 1\n",
    "            else:\n",
    "                break\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_date_spans(tokens: list) -> list:\n",
    "    candidates = []\n",
    "    poss = [token.pos_ for token in tokens]\n",
    "    for idx in range(1, len(tokens)-1):\n",
    "        if poss[idx-1: idx+2] == ['NUM', 'PROPN', 'NUM']:\n",
    "            # Day, month, year\n",
    "            candidates.append(tokens[idx-1: idx+2])\n",
    "        elif idx < len(tokens) -2 and poss[idx-1: idx+3] == ['PROPN', 'NUM', 'PUNCT', 'NUM']:\n",
    "            # Month, Day, comma, year\n",
    "            candidates.append(tokens[idx-1: idx+3])\n",
    "        elif poss[idx: idx+2] ==  ['PROPN', 'NUM'] and tokens[idx+1].head == tokens[idx]:\n",
    "            # Month, day\n",
    "            candidates.append(tokens[idx: idx+2])\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_loc_spans(tokens: list) -> list:\n",
    "    candidates = []\n",
    "    poss = [token.pos_ for token in tokens]\n",
    "    for idx in range(len(tokens)-2):\n",
    "        if poss[idx: idx+3] == ['PROPN', 'PUNCT', 'PROPN'] and tokens[idx+2].dep_ == 'appos':\n",
    "            # City, comma, State\n",
    "            candidates.append(tokens[idx: idx+3])\n",
    "        if idx < len(tokens) - 3 and poss[idx: idx+4] == ['PROPN', 'PROPN', 'PUNCT', 'PROPN'] and tokens[idx+3].dep_ == 'appos':\n",
    "            # City1, City2, comma, State\n",
    "            candidates.append(tokens[idx: idx+4])\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Institute], [the, Institute]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = json.load(open('../data/datasets/conll04/conll04_test_clean.json'))\n",
    "data = dataset[203]\n",
    "tokens = parser(Doc(parser.vocab, words=data['tokens']))\n",
    "token_dic, post_token_dic, token_idx_dic = get_reachable_tokens(tokens)\n",
    "get_token_spans(tokens[3], token_dic, token_idx_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are wrongly annotated data such as an extra punctuation following an entity, we need to remove them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = json.load(open('../data/datasets/conll04/conll04_test.json'))\n",
    "# cleaned = clean_data(test_data)\n",
    "# json.dump(cleaned, open('../data/datasets/conll04/conll04_test_clean.json', 'w'))\n",
    "# test_data = json.load(open('../data/datasets/conll04/conll04_train_dev.json'))\n",
    "# cleaned = clean_data(test_data)\n",
    "# json.dump(cleaned, open('../data/datasets/conll04/conll04_train_dev_clean.json', 'w'))\n",
    "\n",
    "test_data = json.load(open('../data/datasets/scierc/scierc_test.json'))\n",
    "cleaned = clean_data(test_data)\n",
    "json.dump(cleaned, open('../data/datasets/scierc/scierc_test_clean.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code detect the POS of the last/first word in each entity span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(open('../data/datasets/conll04/conll04_test_clean.json'))\n",
    "data = json.load(open('../data/datasets/conll04/conll04_train_dev_clean.json'))\n",
    "\n",
    "poss = {}\n",
    "lens = {}\n",
    "puncts = {}\n",
    "\n",
    "invalid_count = 0\n",
    "adj_count = 0\n",
    "\n",
    "for idx, cur_d in enumerate(data):\n",
    "    words = cur_d['tokens']\n",
    "    doc = Doc(parser.vocab, words=words)\n",
    "    tokens = parser(doc)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.pos_ == 'ADJ' and token.text.isupper():\n",
    "            print('found capitalized adj --->', idx, token.text)\n",
    "            adj_count += 1\n",
    "\n",
    "    for ent in cur_d['entities']:\n",
    "        # pos = tokens[ent['start']: ent['end']][-1].pos_\n",
    "        pos = tokens[ent['start']: ent['end']][0].pos_\n",
    "        ent_words = cur_d['tokens'][ent['start']: ent['end']]\n",
    "\n",
    "        for token in tokens[ent['start']: ent['end']]:\n",
    "            if token.pos_ == 'PUNCT':\n",
    "                punct = token.text\n",
    "                if punct not in puncts:\n",
    "                    puncts[punct] = []\n",
    "                puncts[punct].append(' '.join(ent_words))\n",
    "\n",
    "        if pos not in poss:\n",
    "            poss[pos] = []\n",
    "        poss[pos].append(' '.join(ent_words))\n",
    "        lens[len(ent_words)] = lens.get(len(ent_words), 0) + 1\n",
    "\n",
    "print('invalid ---->', invalid_count)\n",
    "print('capitalized adj ---->', adj_count)\n",
    "print('puncts in entities ------>', puncts)\n",
    "# print(poss)\n",
    "print(lens)\n",
    "json.dump(poss, open('startposs.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show parsing results of a document with extracted spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre token dic-------> {Our: [Our], goal: [goal, Our], is: [is, goal, Our], to: [to], recognize: [recognize, to], and: [and], isolate: [isolate], such: [such], miscommunications: [miscommunications, such], and: [and], circumvent: [circumvent], them: [them], .: [.]}\n",
      "post token dic -----> {Our: [Our], goal: [goal], is: [is, to, recognize, and, isolate, such, miscommunications, and, circumvent, them, .], to: [to], recognize: [recognize, and, isolate, such, miscommunications, and, circumvent, them], and: [and], isolate: [isolate, such, miscommunications, and, circumvent, them], such: [such], miscommunications: [miscommunications], and: [and], circumvent: [circumvent, them], them: [them], .: [.]}\n",
      "extracted spans -----> {(0, 1), (1, 2), (7, 9), (11, 12), (0, 2), (8, 9)}\n",
      "extracted spans -----> [['Our'], ['Our', 'goal'], ['goal'], ['such', 'miscommunications'], ['miscommunications'], ['them']]\n",
      "entities ---> [['miscommunications'], ['them']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f146ae3624c6436fbb169f2244fa1d0e-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Our</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">goal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">recognize</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">isolate</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">such</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">miscommunications</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">circumvent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">them .</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-5\" stroke-width=\"2px\" d=\"M770,352.0 C770,177.0 1090.0,177.0 1090.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,354.0 L1098.0,342.0 1082.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-7\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,177.0 1440.0,177.0 1440.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,354.0 L1448.0,342.0 1432.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-8\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,89.5 1620.0,89.5 1620.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,354.0 L1628.0,342.0 1612.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-9\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,2.0 1800.0,2.0 1800.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,354.0 L1808.0,342.0 1792.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f146ae3624c6436fbb169f2244fa1d0e-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f146ae3624c6436fbb169f2244fa1d0e-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1960.0,354.0 L1968.0,342.0 1952.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = json.load(open('../data/datasets/conll04/conll04_test_clean.json'))\n",
    "dataset = json.load(open('../data/datasets/scierc/scierc_test_clean.json'))\n",
    "data = dataset[294]\n",
    "words = data['tokens']\n",
    "doc = Doc(parser.vocab, words=words)\n",
    "tokens = parser(doc)\n",
    "token_dic, post_token_dic, token_idx_dic = get_reachable_tokens(tokens)\n",
    "print('pre token dic------->', token_dic)\n",
    "print('post token dic ----->', post_token_dic)\n",
    "spans = get_ent_spans(tokens)\n",
    "print('extracted spans ----->', spans)\n",
    "print('extracted spans ----->', [words[ent[0]: ent[1]] for ent in sorted(spans)])\n",
    "print('entities --->', [words[ent['start']: ent['end']] for ent in data['entities']])\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to compute how many entity spans are covered in extracted spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551/551 [00:24<00:00, 22.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data -------------------------------> 551\n",
      "total tokens -----------------------------> 13404\n",
      "total ents -------------------------------> 1685\n",
      "original total spans with max length 10 --> 109245\n",
      "new total spans --------------------------> 10153\n",
      "new total long spans --------------------------> 5342\n",
      "covered ents -----------------------------> 1475\n",
      "cover rate -------------------------------> 87.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = json.load(open('../data/datasets/conll04/conll04_test_clean.json'))\n",
    "# dataset = json.load(open('../data/datasets/conll04/conll04_train_clean.json'))\n",
    "dataset = json.load(open('../data/datasets/scierc/scierc_test_clean.json'))\n",
    "\n",
    "total_ents = 0\n",
    "total_tokens = 0\n",
    "total_spans = 0\n",
    "total_long_spans = 0\n",
    "org_total_spans = 0\n",
    "total_covered_ents = 0\n",
    "uncovered_list = []\n",
    "extrated_poss = {}\n",
    "for idx, data in enumerate(tqdm(dataset)):\n",
    "    tokens = parse_data(data)\n",
    "    org_total_spans += 10 * (len(tokens) + len(tokens) - 10 + 1) // 2\n",
    "    spans = get_ent_spans(tokens)\n",
    "    for span_s, span_e in spans:\n",
    "        start_pos = tokens[span_s].pos_\n",
    "        if start_pos not in extrated_poss:\n",
    "            extrated_poss[start_pos] = []\n",
    "        extrated_poss[start_pos].append(' '.join([token.text for token in tokens[span_s: span_e]]))\n",
    "\n",
    "        if span_e - span_s > 1:\n",
    "            total_long_spans += 1\n",
    "\n",
    "    ent_spans = [(ent['start'], ent['end']) for ent in data['entities']]\n",
    "    total_spans += len(spans)\n",
    "    total_ents += len(ent_spans)\n",
    "    total_tokens += len(data['tokens'])\n",
    "    covered_ents = set(ent_spans).intersection(spans)\n",
    "    total_covered_ents += len(covered_ents)\n",
    "\n",
    "    if len(covered_ents) != len(ent_spans):\n",
    "        uncovered = set(ent_spans) - spans\n",
    "        # print('not fully covered ---> ', idx, uncovered)\n",
    "        for ent_s, ent_e in uncovered:\n",
    "            # print('====', tokens[ent_s: ent_e])\n",
    "            uncovered_list.append(str(idx)+ ': ' +' '.join([token.text for token in tokens[ent_s: ent_e]]))\n",
    "\n",
    "print('total data ------------------------------->', len(dataset))\n",
    "print('total tokens ----------------------------->', total_tokens)\n",
    "print('total ents ------------------------------->', total_ents)\n",
    "print('original total spans with max length 10 -->', org_total_spans)\n",
    "print('new total spans -------------------------->', total_spans)\n",
    "print('new total long spans -------------------------->', total_long_spans)\n",
    "print('covered ents ----------------------------->', total_covered_ents)\n",
    "print('cover rate ------------------------------->', round(total_covered_ents / total_ents * 100, 2), '%')\n",
    "\n",
    "json.dump(extrated_poss, open('extract_start_poss.json', 'w'))\n",
    "json.dump(uncovered_list, open('uncovered.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2723fd6360627580836bdb3cee1e3003e73373d537f1b73543755c25c08e8b1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
