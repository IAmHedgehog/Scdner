{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(data, preds):\n",
    "    data_entities = []\n",
    "\n",
    "    for cur_d in data:\n",
    "        cur_dict = {}\n",
    "        for ent in cur_d['entities']:\n",
    "            cur_dict[(ent['start'], ent['end'])] = ent['type']\n",
    "        data_entities.append(cur_dict)\n",
    "\n",
    "    pred_entities = []\n",
    "\n",
    "    for cur_p in preds:\n",
    "        cur_dict = {}\n",
    "        for ent in cur_p['entities']:\n",
    "            cur_dict[(ent['start'], ent['end'])] = ent['type']\n",
    "        pred_entities.append(cur_dict)\n",
    "    return data_entities, pred_entities\n",
    "\n",
    "\n",
    "def analyze_entities(data_entities, pred_entities):\n",
    "    # cb: correct boundary\n",
    "    # wb: wrong boundary\n",
    "    # ct: correct type\n",
    "    # wt: wrong type\n",
    "\n",
    "    correct = []\n",
    "    missing = []\n",
    "    extra = []\n",
    "    cb_wt = []\n",
    "    wb_ct = []\n",
    "    wb_wt = []\n",
    "\n",
    "    for doc_id, (data_ent, pred_ent) in enumerate(zip(data_entities, pred_entities)):\n",
    "\n",
    "        cbs = set(data_ent.keys()) & set(pred_ent.keys())\n",
    "        for cb in cbs:\n",
    "            if data_ent[cb] == pred_ent[cb]:\n",
    "                correct.append([doc_id, cb, cb, data_ent[cb], pred_ent[cb]])\n",
    "            else:\n",
    "                cb_wt.append({'doc_id': doc_id, 'data_cb': cb, 'true_type': data_ent[cb], 'pred_type': pred_ent[cb]})\n",
    "\n",
    "        data_wbs = set(data_ent.keys()) - cbs\n",
    "        pred_wbs = set(pred_ent.keys()) - cbs\n",
    "\n",
    "        used_cb = set()\n",
    "\n",
    "        for b_s, b_e in pred_wbs:\n",
    "            flag = True\n",
    "            for d_s, d_e in data_wbs:\n",
    "                if min([d_e, b_e]) > max([d_s, b_s]):\n",
    "                    if pred_ent[(b_s, b_e)] == data_ent[(d_s, d_e)]:\n",
    "                        wb_ct.append({'doc_id': doc_id, 'pre_cb': (b_s, b_e), 'data_cb': (d_s, d_e), 'true_type': data_ent[(d_s, d_e)]})\n",
    "                    else:\n",
    "                        wb_wt.append({'doc_id': doc_id, 'pre_cb': (b_s, b_e), 'data_cb': (d_s, d_e), 'true_type': data_ent[(d_s, d_e)], 'pred_type': pred_ent[(b_s, b_e)]})\n",
    "                    flag = False\n",
    "                    used_cb.add((d_s, d_e))\n",
    "                    break\n",
    "            if flag:\n",
    "                extra.append({'doc_id': doc_id, 'pred_cb': (b_s, b_e), 'pred_type': pred_ent[(b_s, b_e)]})\n",
    "        \n",
    "        for cb in data_wbs - used_cb:\n",
    "            missing.append({'doc_id': doc_id, 'data_cb': cb, 'true_type': data_ent[cb]})\n",
    "        \n",
    "    return {'correct': correct, 'missing': missing, 'extra': extra, 'cb_wt': cb_wt, 'wb_ct': wb_ct, 'wb_wt': wb_wt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 10276\n",
      "missing 350\n",
      "extra 473\n",
      "cb_wt 228\n",
      "wb_ct 327\n",
      "wb_wt 131\n",
      "results --------> 10276 1036 1159 0.8986445124617403 0.9084158415841584 0.903503758737416\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# data = json.load(open('../data/datasets/scierc/scierc_test.json'))\n",
    "# preds = json.load(open('../data/log/scierc_train/baseline/predictions_valid_epoch_20.json'))\n",
    "\n",
    "# data = json.load(open('../data/datasets/scierc/scierc_test_new_doc.json'))\n",
    "# preds = json.load(open('../data/log/scierc_train/doc_baseline/predictions_valid_epoch_27.json'))\n",
    "# preds = json.load(open('../data/log/scierc_train/2022-06-24_22:29:36.379915/predictions_valid_epoch_30.json'))\n",
    "\n",
    "\n",
    "# data = json.load(open('../data/datasets/conll04/conll04_test.json'))\n",
    "# preds = json.load(open('../data/log/conll04_train/baseline/predictions_valid_epoch_20.json'))\n",
    "\n",
    "data = json.load(open('../data/datasets/ontonotes/doc_test.json'))\n",
    "preds = json.load(open('../data/log/ontonotes_train/2022-07-04_14:36:41.174231/predictions_valid_epoch_30.json'))\n",
    "\n",
    "data_entities, pred_entities = get_entities(data, preds)\n",
    "results = analyze_entities(data_entities, pred_entities)\n",
    "\n",
    "print('correct', len(results['correct']))\n",
    "print('missing', len(results['missing']))\n",
    "print('extra', len(results['extra']))\n",
    "print('cb_wt', len(results['cb_wt']))\n",
    "print('wb_ct', len(results['wb_ct']))\n",
    "print('wb_wt', len(results['wb_wt']))\n",
    "\n",
    "tp = len(results['correct'])\n",
    "fn = len(results['missing']) + len(results['cb_wt']) + len(results['wb_ct']) + len(results['wb_wt'])\n",
    "fp = len(results['extra']) + len(results['cb_wt']) + len(results['wb_ct']) + len(results['wb_wt'])\n",
    "\n",
    "# tp = len(correct) + len(cb_wt)\n",
    "# fn = len(missing) + len(wb_ct) + len(wb_wt)\n",
    "# fp = len(extra) + len(wb_ct) + len(wb_wt)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "print('results -------->', tp, fn, fp, precision, recall, f1)\n",
    "\n",
    "json.dump(results['extra'], open('analysisResults/extra.json', 'w'))\n",
    "json.dump(results['missing'], open('analysisResults/missing.json', 'w'))\n",
    "json.dump(results['wb_ct'], open('analysisResults/wb_ct.json', 'w'))\n",
    "json.dump(results['wb_wt'], open('analysisResults/wb_wt.json', 'w'))\n",
    "json.dump(results['cb_wt'], open('analysisResults/cb_wt.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing stat --------> {'ORG': 52, 'DATE': 72, 'TIME': 32, 'CARDINAL': 69, 'GPE': 10, 'QUANTITY': 3, 'PERSON': 44, 'ORDINAL': 10, 'NORP': 8, 'LOC': 7, 'WORK_OF_ART': 21, 'MONEY': 3, 'PRODUCT': 8, 'FAC': 3, 'LAW': 3, 'PERCENT': 2, 'LANGUAGE': 1, 'EVENT': 2}\n",
      "extra stat --------> {'TIME': 32, 'CARDINAL': 96, 'QUANTITY': 12, 'DATE': 123, 'GPE': 12, 'ORDINAL': 28, 'PERSON': 33, 'ORG': 56, 'NORP': 16, 'LOC': 6, 'WORK_OF_ART': 14, 'PRODUCT': 8, 'FAC': 1, 'MONEY': 15, 'LAW': 4, 'PERCENT': 13, 'EVENT': 4}\n",
      "wrong boundary correct type stat --------> {'TIME': 25, 'EVENT': 6, 'GPE': 18, 'FAC': 5, 'DATE': 86, 'ORG': 45, 'CARDINAL': 40, 'QUANTITY': 11, 'PERSON': 20, 'NORP': 3, 'LAW': 3, 'PERCENT': 27, 'MONEY': 23, 'PRODUCT': 3, 'WORK_OF_ART': 4, 'LOC': 8}\n",
      "Correct boundary wrong type stat --------> {('CARDINAL', 'PERSON'): 1, ('CARDINAL', 'DATE'): 6, ('GPE', 'ORG'): 21, ('CARDINAL', 'TIME'): 1, ('GPE', 'LOC'): 17, ('ORG', 'WORK_OF_ART'): 32, ('DATE', 'TIME'): 7, ('LOC', 'ORG'): 6, ('EVENT', 'WORK_OF_ART'): 2, ('FAC', 'GPE'): 5, ('NORP', 'ORG'): 6, ('EVENT', 'PERSON'): 3, ('LOC', 'NORP'): 5, ('DATE', 'ORDINAL'): 1, ('FAC', 'ORG'): 15, ('EVENT', 'ORG'): 2, ('FAC', 'LOC'): 7, ('ORG', 'PERSON'): 18, ('DATE', 'ORG'): 3, ('DATE', 'EVENT'): 3, ('GPE', 'PERSON'): 11, ('PERSON', 'WORK_OF_ART'): 4, ('LANGUAGE', 'NORP'): 7, ('LOC', 'WORK_OF_ART'): 1, ('ORDINAL', 'ORG'): 1, ('PRODUCT', 'WORK_OF_ART'): 2, ('ORG', 'PRODUCT'): 10, ('CARDINAL', 'MONEY'): 2, ('GPE', 'NORP'): 6, ('GPE', 'LANGUAGE'): 2, ('LAW', 'ORG'): 2, ('GPE', 'WORK_OF_ART'): 2, ('DATE', 'LAW'): 1, ('LAW', 'ORDINAL'): 6, ('LAW', 'MONEY'): 1, ('DATE', 'WORK_OF_ART'): 1, ('EVENT', 'GPE'): 1, ('MONEY', 'QUANTITY'): 1, ('LAW', 'PRODUCT'): 1, ('NORP', 'PERSON'): 2, ('FAC', 'PERSON'): 1, ('EVENT', 'FAC'): 1, ('FAC', 'WORK_OF_ART'): 1}\n",
      "Wrong boundary wrong type stat --------> {('DATE', 'TIME'): 9, ('DATE', 'ORDINAL'): 8, ('FAC', 'ORG'): 2, ('GPE', 'LOC'): 10, ('CARDINAL', 'QUANTITY'): 4, ('LAW', 'ORDINAL'): 1, ('CARDINAL', 'FAC'): 1, ('CARDINAL', 'DATE'): 9, ('ORG', 'WORK_OF_ART'): 3, ('PERSON', 'WORK_OF_ART'): 12, ('ORG', 'PERSON'): 14, ('NORP', 'ORG'): 6, ('DATE', 'PERSON'): 1, ('LOC', 'NORP'): 1, ('CARDINAL', 'PRODUCT'): 2, ('GPE', 'ORG'): 3, ('FAC', 'GPE'): 4, ('ORG', 'PRODUCT'): 3, ('GPE', 'NORP'): 7, ('CARDINAL', 'TIME'): 3, ('GPE', 'PRODUCT'): 1, ('DATE', 'ORG'): 1, ('CARDINAL', 'WORK_OF_ART'): 1, ('EVENT', 'NORP'): 2, ('FAC', 'LOC'): 1, ('CARDINAL', 'MONEY'): 2, ('DATE', 'EVENT'): 1, ('PERSON', 'TIME'): 2, ('EVENT', 'GPE'): 1, ('LAW', 'ORG'): 1, ('GPE', 'WORK_OF_ART'): 1, ('GPE', 'LAW'): 1, ('NORP', 'WORK_OF_ART'): 1, ('DATE', 'LAW'): 1, ('QUANTITY', 'WORK_OF_ART'): 2, ('MONEY', 'QUANTITY'): 5, ('GPE', 'MONEY'): 1, ('CARDINAL', 'LOC'): 1, ('LOC', 'WORK_OF_ART'): 1, ('FAC', 'WORK_OF_ART'): 1}\n"
     ]
    }
   ],
   "source": [
    "missing_stat = {}\n",
    "for rec in results['missing']:\n",
    "    t_type = rec['true_type']\n",
    "    missing_stat[t_type] = missing_stat.get(t_type, 0) + 1\n",
    "\n",
    "print('missing stat -------->', missing_stat)\n",
    "\n",
    "extra_stat = {}\n",
    "for rec in results['extra']:\n",
    "    t_type = rec['pred_type']\n",
    "    extra_stat[t_type] = extra_stat.get(t_type, 0) + 1\n",
    "\n",
    "print('extra stat -------->', extra_stat)\n",
    "\n",
    "wb_ct_stat = {}\n",
    "for rec in results['wb_ct']:\n",
    "    t_type = rec['true_type']\n",
    "    wb_ct_stat[t_type] = wb_ct_stat.get(t_type, 0) + 1\n",
    "\n",
    "print('wrong boundary correct type stat -------->', wb_ct_stat)\n",
    "\n",
    "cb_wt_stat = {}\n",
    "for rec in results['cb_wt']:\n",
    "    t_type = tuple(sorted([rec['true_type'], rec['pred_type']]))\n",
    "    cb_wt_stat[t_type] = cb_wt_stat.get(t_type, 0) + 1\n",
    "\n",
    "print('Correct boundary wrong type stat -------->', cb_wt_stat)\n",
    "\n",
    "wb_wt_stat = {}\n",
    "for rec in results['wb_wt']:\n",
    "    t_type = tuple(sorted([rec['true_type'], rec['pred_type']]))\n",
    "    wb_wt_stat[t_type] = wb_wt_stat.get(t_type, 0) + 1\n",
    "\n",
    "print('Wrong boundary wrong type stat -------->', wb_wt_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9140801109762442, 0.9373221906116643, 0.9255552629268721, None)\n",
      "0.9970328247429978\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import metrics\n",
    "\n",
    "# probs = json.load(open('../data/log/scierc_train/2022-06-24_23:21:02.651449/predictions_valid_epoch_30.json_b_probs'))\n",
    "probs = json.load(open('../data/log/ontonotes_train/2022-07-04_14:36:41.174231/predictions_valid_epoch_29.json_b_probs'))\n",
    "\n",
    "all_probs = []\n",
    "for prob in probs:\n",
    "    all_probs.extend(prob)\n",
    "\n",
    "probs = [p[0] for p in all_probs]\n",
    "preds = [int(prob[0] > 0.5) for prob in all_probs]\n",
    "gts = [0 if prob[1] == 0 else 1 for prob in all_probs]\n",
    "\n",
    "print(precision_recall_fscore_support(gts, preds, average='binary'))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(gts, probs)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2723fd6360627580836bdb3cee1e3003e73373d537f1b73543755c25c08e8b1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
