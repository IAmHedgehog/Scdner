{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def read_docs(file_name):\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    docs = []\n",
    "    for line in tqdm(lines):\n",
    "        items = line.strip().split('|')\n",
    "        if len(items) >= 3 and items[1] == 't':\n",
    "            doc_id, _, title = items\n",
    "            docs.append((doc_id, title))\n",
    "    return docs\n",
    "\n",
    "def sim_score(title, tokens):\n",
    "    target = ' '.join(tokens)\n",
    "    str_len = min(len(title), len(target))\n",
    "    return SequenceMatcher(None, title[:str_len], target[:str_len]).ratio()\n",
    "\n",
    "\n",
    "def align(docs, sents):\n",
    "    sent_counter = 0\n",
    "    doc_id = None\n",
    "    align_file = open('./align_results.txt', 'w')\n",
    "    for doc in tqdm(docs):\n",
    "        cur_doc_id = doc_id\n",
    "        doc_id, title = doc\n",
    "        while sim_score(title, sents[sent_counter]['tokens']) < 0.8:\n",
    "            sents[sent_counter]['doc_id'] = cur_doc_id\n",
    "            sent_counter += 1\n",
    "\n",
    "            if sent_counter >= len(sents):\n",
    "                print('============', doc_id)\n",
    "                print(title)\n",
    "                3 / 0\n",
    "\n",
    "        align_file.write('-------->%s, %s, %s\\n' % (doc_id, sent_counter, sim_score(title, sents[sent_counter]['tokens'])))\n",
    "        align_file.write(' '.join(sents[sent_counter]['tokens']) + '\\n')\n",
    "        align_file.write(title + '\\n\\n')\n",
    "        if sim_score(title, sents[sent_counter]['tokens']) < 0.9:\n",
    "            print('-------->%s, %s, %s' % (doc_id, sent_counter, sim_score(title, sents[sent_counter]['tokens'])))\n",
    "            print(' '.join(sents[sent_counter]['tokens']))\n",
    "            print(title + '\\n')\n",
    "        sents[sent_counter]['doc_id'] = doc_id\n",
    "        sents[sent_counter+1]['doc_id'] = doc_id\n",
    "        sent_counter += 2\n",
    "    align_file.close()\n",
    "\n",
    "    for sent_id in range(sent_counter, len(sents)):\n",
    "        sents[sent_id]['doc_id'] = doc_id\n",
    "    \n",
    "    for sent in sents:\n",
    "        assert 'doc_id' in sent\n",
    "    \n",
    "    doc_sents_dict = {}\n",
    "    for sent in sents:\n",
    "        if sent['doc_id'] not in doc_sents_dict:\n",
    "            doc_sents_dict[sent['doc_id']] = []\n",
    "        doc_sents_dict[sent['doc_id']].append(sent)\n",
    "    \n",
    "    new_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_id, title = doc\n",
    "        doc_sents = doc_sents_dict[doc_id]\n",
    "        relations = []\n",
    "        tokens = []\n",
    "        entities = []\n",
    "        word_diff = [0]\n",
    "        for sent_id, doc_sent in enumerate(doc_sents):\n",
    "            tokens.append(doc_sent['tokens'])\n",
    "            \n",
    "            word_idx = 0\n",
    "            tags = doc_sent['ner_tags']\n",
    "            while word_idx < len(doc_sent['tokens']):\n",
    "                if tags[word_idx] == 1:\n",
    "                    start = word_idx\n",
    "                    end = word_idx + 1\n",
    "                    while end < len(doc_sent['tokens']) and tags[end] == 2:\n",
    "                        end += 1\n",
    "                    entities.append({'start': start + word_diff[-1], 'end': end + word_diff[-1], 'type': 'Disease', 'sent_id': sent_id})\n",
    "                    word_idx = end\n",
    "                else:\n",
    "                    word_idx += 1\n",
    "\n",
    "            word_diff.append(word_diff[-1] + len(doc_sent['tokens']))\n",
    "\n",
    "        new_doc = {'doc_id': doc_id, 'tokens': tokens, 'entities': entities, 'relations': relations}\n",
    "        new_docs.append(new_doc)\n",
    "\n",
    "    return new_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ncbi_disease (/home/hygao/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1807f47cb4f44206a36a41c7828b7e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ------> test test test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1259/1259 [00:00<00:00, 2182980.05it/s]\n",
      " 40%|████      | 40/100 [00:00<00:00, 389.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------->9585606, 433, 0.8842105263157894\n",
      "The hemochromatosis 845 G - - > A and 187 C - - > G mutations : prevalence in non - Caucasian populations .\n",
      "The hemochromatosis 845 G-->A and 187 C-->G mutations: prevalence in non-Caucasian populations.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 398.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ncbi_disease\")\n",
    "\n",
    "# keys = [('train', 'train', 'train'), ('develop', 'validation', 'dev'), ('test', 'test', 'test')]\n",
    "\n",
    "keys = [('test', 'test', 'test')]\n",
    "\n",
    "for file_key, dataset_key, save_key in keys:\n",
    "    print('processing ------>', file_key, dataset_key, save_key)\n",
    "    file_name = '../data/datasets/ncbi_disease/NCBI%sset_corpus.txt' % file_key\n",
    "    docs = read_docs(file_name)\n",
    "    docs.sort()\n",
    "    json.dump(docs, open('../data/datasets/ncbi_disease/%s_title.json' % file_key, 'w'))\n",
    "    sents = dataset[dataset_key]\n",
    "\n",
    "    sents = [sent for sent in sents if len(sent['tokens']) > 0]\n",
    "\n",
    "    json.dump(sents, open('../data/datasets/ncbi_disease/%s.json' % save_key, 'w'))\n",
    "    new_docs = align(docs, sents)\n",
    "    json.dump(new_docs, open('../data/datasets/ncbi_disease/new_doc_%s.json' % save_key, 'w'))\n",
    "\n",
    "# print(num_sents)\n",
    "# print(len(dataset[dataset_key]))\n",
    "\n",
    "# print(line.strip().split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ncbi_disease (/home/hygao/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae3fe485c84a70b352020275b71a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6924/6924 [00:00<00:00, 2054425.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924 593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# file_key = 'develop'\n",
    "file_key = 'train'\n",
    "file_name = '../data/datasets/ncbi_disease/NCBI%sset_corpus.txt' % file_key\n",
    "\n",
    "dataset = load_dataset(\"ncbi_disease\")\n",
    "\n",
    "# docs = []\n",
    "# with open(file_name) as fh:\n",
    "#     lines = fh.readlines()\n",
    "\n",
    "# num_sents = 0\n",
    "# for line in tqdm(lines):\n",
    "#     doc_id, title, abstract = line.strip().split('\\t')\n",
    "#     # sents = list(nlp(abstract).sents)\n",
    "#     # num_sents += len(sents) + 1\n",
    "#     docs.append((doc_id, title, abstract))\n",
    "\n",
    "docs = read_docs(file_name)\n",
    "\n",
    "docs.sort()\n",
    "\n",
    "print(len(dataset['validation']), len(docs))\n",
    "\n",
    "# train_data = []\n",
    "# docs = []\n",
    "# keys = [('train', 'train'), ('dev', 'validation'), ('test', 'test')]\n",
    "\n",
    "# key, org_key = keys[1]\n",
    "\n",
    "# for sent in dataset[org_key]:\n",
    "    \n",
    "#     print(sent)\n",
    "#     docs.append({'tokens': sents, 'entities': entities, 'relations': relations})\n",
    "\n",
    "# print(len(docs))\n",
    "# json.dump(docs, open('../data/datasets/chemdner/new_doc_%s.json' % key, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = '../data/datasets/ncbi_disease/new_doc_train.json'\n",
    "dev_file_name = '../data/datasets/ncbi_disease/new_doc_dev.json'\n",
    "save_file_name = '../data/datasets/ncbi_disease/new_doc_train_dev.json'\n",
    "\n",
    "train_data = json.load(open(train_file_name))\n",
    "dev_data = json.load(open(dev_file_name))\n",
    "\n",
    "json.dump(train_data + dev_data, open(save_file_name, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2723fd6360627580836bdb3cee1e3003e73373d537f1b73543755c25c08e8b1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
