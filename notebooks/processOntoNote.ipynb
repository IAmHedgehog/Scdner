{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:00<00:00, 985.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import json\n",
    "import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "def get_sent_partitions(sents):\n",
    "    tokens_lengths = []\n",
    "    for sent in sents:\n",
    "        token_encoding = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        if not token_encoding:\n",
    "            token_encoding = [tokenizer.convert_tokens_to_ids('[UNK]')]\n",
    "        tokens_lengths.append(len(token_encoding))\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    sum_length = 0\n",
    "    for idx, length in enumerate(tokens_lengths):\n",
    "        if sum_length + length > 300:\n",
    "            partitions.append((start, idx))\n",
    "            start = idx\n",
    "            sum_length = length\n",
    "        else:\n",
    "            sum_length += length\n",
    "    if start < len(tokens_lengths):\n",
    "        partitions.append((start, len(tokens_lengths)))\n",
    "    return partitions\n",
    "\n",
    "\n",
    "# key = 'dev'\n",
    "key = 'train'\n",
    "file_name = '../data/datasets/ontonotes/%s.json' % key\n",
    "\n",
    "data = json.load(open(file_name))\n",
    "\n",
    "docs = []\n",
    "\n",
    "for cur_d in tqdm.tqdm(data):\n",
    "    sents, ents = cur_d['tokens'], cur_d['entities']\n",
    "\n",
    "    partitions = get_sent_partitions(sents)\n",
    "\n",
    "    for part_s, part_e in partitions:\n",
    "        part_sents = sents[part_s: part_e]\n",
    "        part_ents = ents[part_s: part_e]\n",
    "\n",
    "        # print('-------->', part_ents)\n",
    "\n",
    "        part_diff = sum(len(sent) for sent in sents[:part_s])\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for cur_ents in part_ents:\n",
    "            for sent_id, cur_ent in enumerate(cur_ents):\n",
    "                start, end, label = cur_ent\n",
    "                entities.append({\n",
    "                    'start': start - part_diff,\n",
    "                    'end': end - part_diff + 1,\n",
    "                    'type': label,\n",
    "                    'sent_id': sent_id})\n",
    "        docs.append({'tokens': part_sents, 'entities': entities, 'relations': []})\n",
    "print(len(docs))\n",
    "json.dump(docs, open('../data/datasets/ontonotes/doc_%s.json' % key, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 {'ORG', 'PERCENT', 'TIME', 'QUANTITY', 'EVENT', 'MONEY', 'PRODUCT', 'CARDINAL', 'NORP', 'ORDINAL', 'LAW', 'GPE', 'LOC', 'DATE', 'PERSON', 'WORK_OF_ART', 'FAC', 'LANGUAGE'}\n",
      "[9, 9, 11, 9, 10, 11, 12, 13, 10, 9, 12, 13, 21, 18, 9, 15] 11257 0.001421337834236475 21\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "key = 'test'\n",
    "\n",
    "data = json.load(open('../data/datasets/ontonotes/doc_%s.json' % key))\n",
    "keys = set()\n",
    "max_size = 0\n",
    "big = []\n",
    "all = 0\n",
    "\n",
    "for doc in data:\n",
    "    tokens = []\n",
    "    for tks in doc['tokens']:\n",
    "        tokens.extend(tks)\n",
    "    for ent in doc['entities']:\n",
    "        all += 1\n",
    "        size = ent['end'] - ent['start']\n",
    "        if size > 8:\n",
    "            big.append(size)\n",
    "            # print('---------->', ent['end'] - ent['start'])\n",
    "            # print(tokens[ent['start']: ent['end']])\n",
    "        max_size = max(max_size, size)\n",
    "        keys.add(ent['type'])\n",
    "\n",
    "print(len(keys), keys)\n",
    "\n",
    "keys_dict = {}\n",
    "\n",
    "for key in keys:\n",
    "    keys_dict[key] = {'short': key, 'verbose': key}\n",
    "\n",
    "# json.dump({'entities': keys_dict, 'relations': {}}, open('../data/datasets/ontonotes/ontonotes_types.json', 'w'))\n",
    "\n",
    "print(big, all, len(big) / all, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_data = json.load(open('../data/datasets/ontonotes/doc_train.json'))\n",
    "dev_data = json.load(open('../data/datasets/ontonotes/doc_dev.json'))\n",
    "\n",
    "json.dump(train_data+dev_data, open('../data/datasets/ontonotes/doc_train_dev.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2723fd6360627580836bdb3cee1e3003e73373d537f1b73543755c25c08e8b1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
