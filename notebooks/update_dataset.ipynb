{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "50\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for key in ['train', 'dev', 'test']:\n",
    "    docs = []\n",
    "    data = open('../data/datasets/scierc/raw_%s.json' % key)\n",
    "    lines = data.readlines()\n",
    "    for line in lines:\n",
    "        docs.append(json.loads(line))\n",
    "    print(len(lines))\n",
    "    json.dump(docs, open('../data/datasets/scierc/raw_%s_valid.json' % key, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861 1861\n",
      "275 275\n",
      "551 551\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for key in ['train', 'dev', 'test']:\n",
    "\n",
    "    cur_data = json.load(open('../data/datasets/scierc/scierc_%s.json' % key))\n",
    "    raw_data = json.load(open('../data/datasets/scierc/raw_%s_valid.json' % key))\n",
    "\n",
    "    cur_sent_idx = -1\n",
    "    sent_doc_ids = []\n",
    "    for doc_idx, doc in enumerate(raw_data):\n",
    "        sent_doc_ids.extend(len(doc['sentences']) * [doc_idx])\n",
    "\n",
    "    print(len(sent_doc_ids), len(cur_data))\n",
    "    # print(sent_doc_ids)\n",
    "\n",
    "    new_data = {}\n",
    "    for sent_idx, sent in enumerate(cur_data):\n",
    "        cur_doc_id = sent_doc_ids[sent_idx]\n",
    "        if cur_doc_id not in new_data:\n",
    "            new_data[cur_doc_id] = []\n",
    "        new_data[cur_doc_id].append(sent)\n",
    "\n",
    "    new_data_doc = []\n",
    "\n",
    "    for doc_id in range(len(new_data)):\n",
    "        word_shifts = [0]\n",
    "        ent_shifts = [0]\n",
    "        cur_doc = {}\n",
    "        tokens = []\n",
    "        entities = []\n",
    "        relations = []\n",
    "        for sent_idx, sent in enumerate(new_data[doc_id]):\n",
    "            tokens.append(sent['tokens'])\n",
    "            for ent in sent['entities']:\n",
    "                ent['start'] += word_shifts[-1]\n",
    "                ent['end'] += word_shifts[-1]\n",
    "                ent['sent_id'] = sent_idx\n",
    "                entities.append(ent)\n",
    "            for rel in sent['relations']:\n",
    "                rel['head'] += ent_shifts[-1]\n",
    "                rel['tail'] += ent_shifts[-1]\n",
    "                rel['sent_id'] = sent_idx\n",
    "                relations.append(rel)\n",
    "\n",
    "            word_shifts.append(word_shifts[-1]+len(sent['tokens']))\n",
    "            ent_shifts.append(ent_shifts[-1]+len(sent['entities']))\n",
    "        \n",
    "        new_data_doc.append({\n",
    "            'tokens': tokens, 'entities': entities, 'relations': relations})\n",
    "\n",
    "    json.dump(new_data_doc, open('../data/datasets/scierc/scierc_%s_new_doc.json' % key, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check inconsistency in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Material', 'Method', 'Generic', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Generic', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Metric', 'Generic', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'OtherScientificTerm', 'Method', 'Generic', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Generic', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Material', 'Generic', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Generic'}\n",
      " {'OtherScientificTerm', 'Material', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'OtherScientificTerm', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Method', 'Task'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Generic', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Metric', 'Task'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Generic', 'Method'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Material', 'Method', 'Generic'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Metric'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Generic'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Metric', 'Generic', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Generic'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method'}\n",
      " {'Material', 'Generic', 'Method'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method', 'Metric'}\n",
      " {'OtherScientificTerm', 'Method', 'Generic', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'Method', 'Generic', 'Metric'}\n",
      " {'OtherScientificTerm', 'Method'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Material', 'Generic', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Method'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Material', 'Method', 'Task', 'Metric'}\n",
      " {'OtherScientificTerm', 'Method', 'Generic', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Metric', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Generic'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'Generic', 'Metric', 'OtherScientificTerm', 'Material', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Material', 'Method', 'Generic'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Method', 'Metric', 'Task'}\n",
      " {'Generic', 'OtherScientificTerm', 'Material', 'Method', 'Task'}\n",
      " {'OtherScientificTerm', 'Generic', 'Method', 'Task'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# for key in ['train', 'dev', 'test']:\n",
    "key = 'test'\n",
    "data_doc = json.load(open('../data/datasets/scierc/scierc_%s_new_doc.json' % key))\n",
    "\n",
    "inconsistence = []\n",
    "\n",
    "for doc_idx, doc in enumerate(data_doc):\n",
    "    types = {}\n",
    "    for ent in doc['entities']:\n",
    "        word = doc['tokens'][ent['sent_id']][ent['start']: ent['end']]\n",
    "        word = ' '.join(word)\n",
    "        if word not in types:\n",
    "            types[word] = []\n",
    "        types[word].append(ent['type'])\n",
    "    \n",
    "    for word, keys in types.items():\n",
    "        if len(set(keys)) != 1:\n",
    "            inconsistence.append({'doc': doc_idx, 'word': word, 'types': types})\n",
    "\n",
    "\n",
    "print(len(inconsistence))\n",
    "for record in inconsistence:\n",
    "    # print(record['doc'])\n",
    "    for word, keys in record['types'].items():\n",
    "        if len(set(keys)) != 1:\n",
    "            print(word, set(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "16\n",
      "robust PCA {'Method', 'Task'}\n",
      "61\n",
      "PER {'OtherScientificTerm', 'Metric'}\n",
      "69\n",
      "discourse {'OtherScientificTerm', 'Material'}\n",
      "75\n",
      "word list {'OtherScientificTerm', 'Material'}\n",
      "94\n",
      "text input {'OtherScientificTerm', 'Material'}\n",
      "entities with mentions > 1:  1918\n",
      "average number of mentions:  1.1025418615175744\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open('../data/datasets/scierc/scierc_test_new_doc.json'))\n",
    "preds = json.load(open('../data/log/scierc_train/doc_baseline/predictions_valid_epoch_27.json'))\n",
    "\n",
    "\n",
    "inconsistence = []\n",
    "all_records = []\n",
    "\n",
    "for doc_idx, doc in enumerate(preds):\n",
    "    tokens = []\n",
    "    for ts in data[doc_idx]['tokens']:\n",
    "        tokens.extend(ts)\n",
    "\n",
    "    types = {}\n",
    "    for ent in doc['entities']:\n",
    "        word = tokens[ent['start']: ent['end']]\n",
    "        word = ' '.join(word)\n",
    "        if word not in types:\n",
    "            types[word] = []\n",
    "        types[word].append(ent['type'])\n",
    "    \n",
    "    for word, keys in types.items():\n",
    "        if len(set(keys)) != 1:\n",
    "            inconsistence.append({'doc': doc_idx, 'types': types})\n",
    "        all_records.append({'doc': doc_idx, 'types': types})\n",
    "\n",
    "\n",
    "print(len(inconsistence))\n",
    "for record in inconsistence:\n",
    "    print(record['doc'])\n",
    "    for word, keys in record['types'].items():\n",
    "        if len(set(keys)) != 1:\n",
    "            print(word, set(keys))\n",
    "\n",
    "mentions = []\n",
    "for record in all_records:\n",
    "    for keys in record['types'].values():\n",
    "        mentions.append(keys)\n",
    "\n",
    "print('entities with mentions > 1: ', len([keys for keys in mentions if len(keys)>1]))\n",
    "print('average number of mentions: ', sum(len(keys) for keys in mentions) /len(mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_data = json.load(open('../data/datasets/scierc/scierc_train_new_doc.json'))\n",
    "dev_data = json.load(open('../data/datasets/scierc/scierc_dev_new_doc.json'))\n",
    "\n",
    "train_data.extend(dev_data)\n",
    "\n",
    "json.dump(train_data, open('../data/datasets/scierc/scierc_tain_dev_new_doc.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2723fd6360627580836bdb3cee1e3003e73373d537f1b73543755c25c08e8b1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
